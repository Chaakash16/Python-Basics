{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/+e3dS1c8zw78Nl7kz89W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaakash16/Python-Basics/blob/main/KNN_%26_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**"
      ],
      "metadata": {
        "id": "z4zbaMLAf8SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is K-Nearest Neighbors (KNN) and how does it work?**  \n",
        "KNN is a non-parametric algorithm that classifies or predicts a data point based on the majority label or average of its ‘K’ nearest neighbors. It uses distance metrics like Euclidean to find closeness.\n",
        "\n",
        "**2. What is the difference between KNN Classification and KNN Regression?**  \n",
        "In KNN classification, the output is the majority class among neighbors, while in KNN regression, the output is the average (or weighted average) of the neighbors’ values.\n",
        "\n",
        "**3. What is the role of the distance metric in KNN?**  \n",
        "The distance metric determines how closeness is measured between points. Common metrics include Euclidean, Manhattan, and Minkowski.\n",
        "\n",
        "**4. What is the Curse of Dimensionality in KNN?**  \n",
        "As dimensions increase, data becomes sparse and distance between points becomes less meaningful, reducing KNN’s effectiveness and increasing computation.\n",
        "\n",
        "**5. How can we choose the best value of K in KNN?**  \n",
        "Use cross-validation to test different K values and choose the one that gives the best performance. A smaller K may overfit; a larger K may underfit.\n",
        "\n",
        "**6. What are KD Tree and Ball Tree in KNN?**  \n",
        "They are data structures used to speed up nearest neighbor searches by organizing data hierarchically to avoid brute-force computation.\n",
        "\n",
        "**7. When should you use KD Tree vs. Ball Tree?**  \n",
        "KD Tree works better with low-dimensional data (less than 20 features), while Ball Tree is more efficient with higher-dimensional data.\n",
        "\n",
        "**8. What are the disadvantages of KNN?**  \n",
        "KNN is slow on large datasets, sensitive to irrelevant features and outliers, and struggles with high-dimensional data.\n",
        "\n",
        "**9. How does feature scaling affect KNN?**  \n",
        "Since KNN uses distance metrics, unscaled features can bias the results. Scaling ensures all features contribute equally to distance.\n",
        "\n",
        "**10. What is PCA (Principal Component Analysis)?**  \n",
        "PCA is a dimensionality reduction technique that transforms data into new axes (principal components) capturing maximum variance.\n",
        "\n",
        "**11. How does PCA work?**  \n",
        "PCA standardizes the data, computes the covariance matrix, then finds eigenvectors (directions) and eigenvalues (variance) to project data onto fewer dimensions.\n",
        "\n",
        "**12. What is the geometric intuition behind PCA?**  \n",
        "PCA rotates the coordinate system to align with the directions of greatest variance, so the first few axes (principal components) capture most of the information.\n",
        "\n",
        "**13. What is the difference between Feature Selection and Feature Extraction?**  \n",
        "Feature selection picks a subset of original features; feature extraction creates new features from combinations of original ones (like in PCA).\n",
        "\n",
        "**14. What are Eigenvalues and Eigenvectors in PCA?**  \n",
        "Eigenvectors represent the directions of new axes, while eigenvalues represent how much variance each principal component explains.\n",
        "\n",
        "**15. How do you decide the number of components to keep in PCA?**  \n",
        "Use a scree plot or retain enough components to explain a desired percentage (e.g., 95%) of total variance.\n",
        "\n",
        "**16. Can PCA be used for classification?**  \n",
        "Yes, PCA can be used for dimensionality reduction before classification to improve performance and reduce overfitting.\n",
        "\n",
        "**17. What are the limitations of PCA?**  \n",
        "PCA assumes linearity, may lose interpretability, and doesn’t work well if data variance doesn’t align with important features.\n",
        "\n",
        "**18. How do KNN and PCA complement each other?**  \n",
        "PCA reduces dimensionality and noise, which improves KNN’s accuracy and reduces computation, especially in high-dimensional spaces.\n",
        "\n",
        "**19. How does KNN handle missing values in a dataset?**  \n",
        "KNN doesn’t handle missing values natively; imputation techniques are needed beforehand, or distance-based methods can estimate missing values.\n",
        "\n",
        "**20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?**  \n",
        "PCA is unsupervised and maximizes variance, while LDA is supervised and maximizes class separability using label information.\n"
      ],
      "metadata": {
        "id": "PgEQ8CnXf8PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "J8iZpHHvf8Mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a KNN Classifier on the Iris dataset and print model accuracy"
      ],
      "metadata": {
        "id": "3XWgKanaf8KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "hQFoMkxIjon0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "Sjc4jHbUf76-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "JpHYcjVpjzsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy"
      ],
      "metadata": {
        "id": "8f-z4_i7jXSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=3, metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    acc = knn.score(X_test, y_test)\n",
        "    print(f\"{metric.capitalize()} Distance Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "id": "RoB7ypKej156"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a KNN Classifier with different values of K and visualize decision boundaries"
      ],
      "metadata": {
        "id": "HBJ8BSbhjXs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data[:, :2], iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "for k in [1, 5, 15]:\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(X_train, y_train)\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k')\n",
        "    plt.title(f\"Decision boundary (k={k})\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-s4iobHPj3k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Apply Feature Scaling before training a KNN model and compare results with unscaled data"
      ],
      "metadata": {
        "id": "n6PCVKhfjYkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Accuracy without scaling:\", knn.score(X_test, y_test))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "print(\"Accuracy with scaling:\", knn_scaled.score(X_test_scaled, y_test))\n"
      ],
      "metadata": {
        "id": "mQiEX6Nbj7e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a PCA model on synthetic data and print the explained variance ratio for each component"
      ],
      "metadata": {
        "id": "nAISMLOejYrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X, _ = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "JJkNchvkj9OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA"
      ],
      "metadata": {
        "id": "FLv7vVCBjYwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "print(\"Accuracy with PCA:\", knn_pca.score(X_test_pca, y_test))\n"
      ],
      "metadata": {
        "id": "0yPbgipuj-2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV"
      ],
      "metadata": {
        "id": "bFpNW-YEjYy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "param_grid = {'n_neighbors': [1, 3, 5, 7, 9], 'weights': ['uniform', 'distance']}\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "i25Eks6EkANs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a KNN Classifier and check the number of misclassified samples"
      ],
      "metadata": {
        "id": "FPkYXFG9jY1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn.predict(X_test)\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "print(\"Misclassified samples:\", misclassified)\n"
      ],
      "metadata": {
        "id": "IYuLA986kBsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train a PCA model and visualize the cumulative explained variance"
      ],
      "metadata": {
        "id": "cv7DdRQ9jY4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "pca = PCA().fit(X_train)\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "834leMGUkC69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy"
      ],
      "metadata": {
        "id": "4H7yIThbjY7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "for weights in ['uniform', 'distance']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, weights=weights)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"Accuracy with {weights} weights:\", knn.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "ZQXzlaGPk4iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a KNN Regressor and analyze the effect of different K values on performance"
      ],
      "metadata": {
        "id": "mD_oWdAJjZUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=15, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "for k in [1, 3, 5, 10]:\n",
        "    model = KNeighborsRegressor(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"K={k}, MSE={mean_squared_error(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "pTKHyuQ5k5NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Implement KNN Imputation for handling missing values in a dataset"
      ],
      "metadata": {
        "id": "ulBKODjmjZXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "print(X_imputed)\n"
      ],
      "metadata": {
        "id": "O0wnlJssk6wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a PCA model and visualize the data projection onto the first two principal components"
      ],
      "metadata": {
        "id": "L3HFt4bSjZZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_train)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)\n",
        "plt.title(\"Data projected onto first two PCA components\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QEZ2SlPtk7OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance"
      ],
      "metadata": {
        "id": "KHw9WZCMkXaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for algo in ['kd_tree', 'ball_tree']:\n",
        "    knn = KNeighborsClassifier(algorithm=algo)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"{algo} accuracy:\", knn.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "BR9GM8J_k8uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot"
      ],
      "metadata": {
        "id": "ldz9HY82kXzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X_hd, _ = make_classification(n_samples=200, n_features=50, random_state=0)\n",
        "pca = PCA().fit(X_hd)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Components')\n",
        "plt.ylabel('Cumulative Variance')\n",
        "plt.title('Scree Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3YV28HENk9Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score"
      ],
      "metadata": {
        "id": "jpzpoPrukX2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "E3Kzlo43k94R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a PCA model and analyze the effect of different numbers of components on accuracy"
      ],
      "metadata": {
        "id": "2CYtGKcOkX43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [2, 3, 4]:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "    print(f\"Accuracy with {n} components:\", knn.score(X_test_pca, y_test))\n"
      ],
      "metadata": {
        "id": "6A0wGBpzk-ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a KNN Classifier with different leaf_size values and compare accuracy"
      ],
      "metadata": {
        "id": "cHxde1BbkX7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for leaf in [10, 30, 50]:\n",
        "    knn = KNeighborsClassifier(leaf_size=leaf)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"Leaf size {leaf} accuracy:\", knn.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "LnH4dLJGk-3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a PCA model and visualize how data points are transformed before and after PCA"
      ],
      "metadata": {
        "id": "PRHxJFO6kX-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
        "plt.title(\"Original Data\")\n",
        "\n",
        "X_pca = PCA(n_components=2).fit_transform(X_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)\n",
        "plt.title(\"After PCA\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h5_O64Ywk_Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report"
      ],
      "metadata": {
        "id": "kmwZkr3rkYA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "F9oPPhqBk_5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error"
      ],
      "metadata": {
        "id": "A8ZsGXQYkYDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsRegressor(metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    print(f\"{metric} MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "299t5AeBlAXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a KNN Classifier and evaluate using ROC-AUC score"
      ],
      "metadata": {
        "id": "0XD1tEn7kYGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
        "y_score = knn.predict_proba(X_test)\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_bin, y_score, multi_class='ovr'))\n"
      ],
      "metadata": {
        "id": "6PM98OWwlQAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a PCA model and visualize the variance captured by each principal component"
      ],
      "metadata": {
        "id": "3Yz1tsrOkYIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA().fit(X_train)\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
        "plt.xlabel(\"Components\")\n",
        "plt.ylabel(\"Variance Ratio\")\n",
        "plt.title(\"Explained Variance per Component\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N9YVzb-2lQp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a KNN Classifier and perform feature selection before training"
      ],
      "metadata": {
        "id": "CDzDOIOqkrsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "X_new = SelectKBest(score_func=f_classif, k=5).fit_transform(X_train, y_train)\n",
        "X_test_new = SelectKBest(score_func=f_classif, k=5).fit(X_train, y_train).transform(X_test)\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_new, y_train)\n",
        "print(\"Accuracy after feature selection:\", knn.score(X_test_new, y_test))\n"
      ],
      "metadata": {
        "id": "pLiDrm0qlRPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Train a PCA model and visualize the data reconstruction error after reducing dimensions"
      ],
      "metadata": {
        "id": "2T41coRNksBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=5)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "X_reconstructed = pca.inverse_transform(X_reduced)\n",
        "error = np.mean((X_train - X_reconstructed) ** 2)\n",
        "print(\"Reconstruction Error:\", error)\n"
      ],
      "metadata": {
        "id": "NniWtmF3lSXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. Train a KNN Classifier and visualize the decision boundary"
      ],
      "metadata": {
        "id": "KH6DgmCqksG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X, y = iris.data[:, :2], iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k')\n",
        "plt.title(\"Decision Boundary (k=5)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0FEY4H0DlS07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. Train a PCA model and analyze the effect of different numbers of components on data variance"
      ],
      "metadata": {
        "id": "3OW2GfJmksMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA().fit(X_train)\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Variance')\n",
        "plt.title('PCA Variance Analysis')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ecd3M8JvlTON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}