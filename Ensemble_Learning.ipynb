{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGGXZ+8h/I7iviRnfCrxuK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaakash16/Python-Basics/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**"
      ],
      "metadata": {
        "id": "ui-ziggPU6Bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yeh rahe 20 questions ke sath concise (2–3 line) answers:\n",
        "\n",
        "---\n",
        "\n",
        "**1. Can we use Bagging for regression problems?**  \n",
        "Yes, Bagging can be used for regression. It averages predictions from multiple models to reduce variance and improve accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**2. What is the difference between multiple model training and single model training?**  \n",
        "Single model training uses one algorithm, while multiple model training (ensemble) combines several models to improve performance and reduce errors.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Explain the concept of feature randomness in Random Forest.**  \n",
        "In Random Forest, each tree gets a random subset of features, which reduces correlation between trees and increases model robustness.\n",
        "\n",
        "---\n",
        "\n",
        "**4. What is OOB (Out-of-Bag) Score?**  \n",
        "OOB score is a performance metric calculated using data not included in the bootstrap sample. It helps evaluate model accuracy without a separate validation set.\n",
        "\n",
        "---\n",
        "\n",
        "**5. How can you measure the importance of features in a Random Forest model?**  \n",
        "Feature importance is measured by how much each feature decreases impurity or improves accuracy across trees in the forest.\n",
        "\n",
        "---\n",
        "\n",
        "**6. Explain the working principle of a Bagging Classifier.**  \n",
        "Bagging Classifier trains multiple models on different bootstrapped samples and aggregates their predictions (e.g., by majority vote) to make final decisions.\n",
        "\n",
        "---\n",
        "\n",
        "**7. How do you evaluate a Bagging Classifier’s performance?**  \n",
        "You can use metrics like accuracy, precision, recall, and OOB score on test or validation data to evaluate performance.\n",
        "\n",
        "---\n",
        "\n",
        "**8. How does a Bagging Regressor work?**  \n",
        "Bagging Regressor builds several regression models on bootstrapped samples and averages their outputs for final prediction, reducing variance.\n",
        "\n",
        "---\n",
        "\n",
        "**9. What is the main advantage of ensemble techniques?**  \n",
        "They improve accuracy, reduce overfitting, and handle noisy data better by combining multiple weak or strong models.\n",
        "\n",
        "---\n",
        "\n",
        "**10. What is the main challenge of ensemble methods?**  \n",
        "They are computationally expensive, harder to interpret, and may require more memory and time for training and prediction.\n",
        "\n",
        "---\n",
        "\n",
        "**11. Explain the key idea behind ensemble techniques.**  \n",
        "Ensemble techniques combine predictions from multiple models to make a more accurate and stable prediction than individual models.\n",
        "\n",
        "---\n",
        "\n",
        "**12. What is a Random Forest Classifier?**  \n",
        "It’s an ensemble method using multiple decision trees on random data and features, combining their outputs via majority voting for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**13. What are the main types of ensemble techniques?**  \n",
        "The main types are Bagging, Boosting, and Stacking, each combining multiple models in different ways to enhance performance.\n",
        "\n",
        "---\n",
        "\n",
        "**14. What is ensemble learning in machine learning?**  \n",
        "Ensemble learning is the process of combining several base models to produce a better predictive model.\n",
        "\n",
        "---\n",
        "\n",
        "**15. When should we avoid using ensemble methods?**  \n",
        "Avoid when the dataset is small, training time is critical, or interpretability is important, as ensemble models are complex.\n",
        "\n",
        "---\n",
        "\n",
        "**16. How does Bagging help in reducing overfitting?**  \n",
        "By training on different subsets of data and averaging results, Bagging reduces model variance, thus lowering the risk of overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**17. Why is Random Forest better than a single Decision Tree?**  \n",
        "Random Forest is more accurate and stable because it reduces overfitting by combining multiple trees trained on different data and features.\n",
        "\n",
        "---\n",
        "\n",
        "**18. What is the role of bootstrap sampling in Bagging?**  \n",
        "Bootstrap sampling creates diverse datasets by randomly sampling with replacement, helping the model generalize better.\n",
        "\n",
        "---\n",
        "\n",
        "**19. What are some real-world applications of ensemble techniques?**  \n",
        "They are used in fraud detection, recommendation systems, image classification, medical diagnosis, and stock market predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**20. What is the difference between Bagging and Boosting?**  \n",
        "Bagging trains models independently in parallel to reduce variance, while Boosting trains models sequentially, focusing on correcting previous errors to reduce bias.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6SQKlAIwVApW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "vNHpwlvgVEsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "d-vNws0zVO-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingClassifier with DecisionTreeClassifier on a dataset like Iris or Titanic. Train, predict, and then use accuracy_score to evaluate."
      ],
      "metadata": {
        "id": "hP6L-5FxVcmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "BhvO9pOqVfLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingRegressor with DecisionTreeRegressor, fit it on any regression dataset (like Boston housing), and use mean_squared_error to evaluate."
      ],
      "metadata": {
        "id": "jfegsrHNVguQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "fVoJo90pVie0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Load the Breast Cancer dataset using sklearn.datasets, train a RandomForestClassifier, and use .feature_importances_ to print scores."
      ],
      "metadata": {
        "id": "KyOcwwIoVjyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "5jw1QxTeVl_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train both RandomForestRegressor and DecisionTreeRegressor on the same dataset, and compare their RMSE or R² scores. RF usually performs better."
      ],
      "metadata": {
        "id": "z8aa_4RGVp5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "et6I68F-VsFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train a RandomForestClassifier with oob_score=True. After training, use .oob_score_ to get the validation accuracy without using test data."
      ],
      "metadata": {
        "id": "Z2vXbOWOVtb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "aKzv5QquVvLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingClassifier with SVC() as base estimator. Fit the model on a classification dataset and print accuracy_score on test data."
      ],
      "metadata": {
        "id": "I_VH3gbmVxZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "wa6mf4pNVzIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train RandomForestClassifier with varying n_estimators (e.g., 10, 50, 100, 200) and observe accuracy changes on the same test set."
      ],
      "metadata": {
        "id": "9q4-FMjZV0WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "QqKoJ4z_V17l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingClassifier with LogisticRegression and evaluate using roc_auc_score for binary classification tasks."
      ],
      "metadata": {
        "id": "tRVvm1QRWDxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "P-S2ATCRV6R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "After training RandomForestRegressor, use .feature_importances_ to understand which features contribute most to predictions."
      ],
      "metadata": {
        "id": "T9p0EzPiV4qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Train an ensemble model using both Bagging and Random Forest and compare accuracy"
      ],
      "metadata": {
        "id": "zNsGBNnMWAoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train a BaggingClassifier and a RandomForestClassifier on the same dataset. Compare both models’ accuracy—Random Forest usually outperforms."
      ],
      "metadata": {
        "id": "62NToE7GWG5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "9A-yzHJ1WIoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use GridSearchCV with parameters like n_estimators, max_depth, and min_samples_split. It helps in finding the best combination for highest accuracy."
      ],
      "metadata": {
        "id": "_MaUeg3pWcDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "aYhxowwFWdgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Try n_estimators = 10, 50, 100, and evaluate using MSE or R². Generally, more estimators give better and more stable performance."
      ],
      "metadata": {
        "id": "cTAKKQA5WfrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "VBljrwJjWg-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Compare y_pred vs y_test and extract indices where predictions are wrong. Analyzing these helps identify model weaknesses or confusing data points."
      ],
      "metadata": {
        "id": "R42vPC7TWiWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "JL3X-tdbWjyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bagging improves accuracy by reducing variance. Compare their test set accuracy or F1-score—Bagging is usually more stable and performs better."
      ],
      "metadata": {
        "id": "Z7OEJOVGWnVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "VsdRaSK2Wpiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use confusion_matrix and visualize it with seaborn.heatmap to see correct vs incorrect predictions. Helps in understanding classification performance."
      ],
      "metadata": {
        "id": "q2C38S57W272"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "VpLEaENnW3Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use StackingClassifier with DT, SVM as base models and Logistic Regression as meta-learner. Compare with individual models for better insights."
      ],
      "metadata": {
        "id": "avaQLh6TW4cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "Cx4xKdYdW5pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "After training, use .feature_importances_ and sort the values to print top 5 features contributing the most to predictions."
      ],
      "metadata": {
        "id": "wKZZybr3W7Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "tsFZ1xp1W8U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use precision_score, recall_score, and f1_score to evaluate classification performance beyond accuracy—useful for imbalanced datasets."
      ],
      "metadata": {
        "id": "ptwFnVoOW9xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "X6LdzGA8W-9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train with different max_depth values. Plot depth vs accuracy to find the best depth—shallow may underfit, deeper might overfit."
      ],
      "metadata": {
        "id": "X9C5-b68XA13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
      ],
      "metadata": {
        "id": "SQXW3Z29XASP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use both DecisionTreeRegressor and KNeighborsRegressor inside BaggingRegressor. Compare MSE to see which base model suits better."
      ],
      "metadata": {
        "id": "WljIdKD2XDzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "caIbJiXUXEzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use roc_auc_score from sklearn.metrics. AUC close to 1 means excellent model. Good for binary classification with class imbalance."
      ],
      "metadata": {
        "id": "kI-Qw2u6XGLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Classifier and evaluate its performance using cross-validation"
      ],
      "metadata": {
        "id": "DG4Vhub6XHIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use cross_val_score() with BaggingClassifier to get average performance across folds—gives better generalization estimate."
      ],
      "metadata": {
        "id": "ahcEkH2VXIUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "4lu378noXKAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use precision_recall_curve() and plot the graph using matplotlib. Helps in understanding model performance on different thresholds."
      ],
      "metadata": {
        "id": "fmlQ6I3ZXLdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "wOC2PyTQXMhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Combine RandomForestClassifier and LogisticRegression using StackingClassifier. Usually improves accuracy by combining strengths of both."
      ],
      "metadata": {
        "id": "8W9MUywgXNip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
      ],
      "metadata": {
        "id": "2GA6lGwFXO6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tune max_samples (like 0.5, 0.7, 1.0) and observe how sampling size affects model accuracy and variance using metrics like MSE."
      ],
      "metadata": {
        "id": "FzahSNfsXP5T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}