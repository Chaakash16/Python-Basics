{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAsiEcOp8nnOzIXLRR6VBC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaakash16/Python-Basics/blob/main/Evaluation_Metrics_and_Regression_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**\n",
        "1. What does R-squared represent in a regression model?\n",
        "R-squared represents the proportion of variance in the dependent variable that is explained by the independent variables.\n",
        "\n",
        "2. What are the assumptions of linear regression?\n",
        "\n",
        "*   Linearity\n",
        "*   Independence of errors\n",
        "*   Homoscedasticity (constant error variance)\n",
        "*   Normal distribution of residuals\n",
        "*   No multicollinearity\n",
        "\n",
        "3. What is the difference between R-squared and Adjusted R-squared?\n",
        "Adjusted R-squared penalizes the addition of unnecessary predictors, making it more reliable when comparing models with different numbers of features.\n",
        "\n",
        "4. Why do we use Mean Squared Error (MSE)?\n",
        "MSE is used because it penalizes larger errors more than smaller ones, giving a clear metric for optimization in regression models.\n",
        "\n",
        "5. What does an Adjusted R-squared value of 0.85 indicate?\n",
        "It means 85% of the variance in the dependent variable is explained by the model, accounting for the number of predictors.\n",
        "\n",
        "6. How do we check for normality of residuals in linear regression?\n",
        "\n",
        "*   Use a Q-Q plot or histogram of residuals\n",
        "*   Apply statistical tests like Shapiro-Wilk\n",
        "\n",
        "7. What is multicollinearity, and how does it impact regression?\n",
        "Multicollinearity refers to high correlation between independent variables. It causes unstable estimates and inflates standard errors.\n",
        "\n",
        "8. What is Mean Absolute Error (MAE)?\n",
        "MAE is the average of the absolute differences between predicted and actual values. It is simple and interpretable.\n",
        "\n",
        "9. What are the benefits of using an ML pipeline?\n",
        "\n",
        "*   Automates preprocessing and modeling steps\n",
        "*   Ensures reproducibility\n",
        "*   Reduces chances of data leakage\n",
        "\n",
        "10. Why is RMSE considered more interpretable than MSE?\n",
        "RMSE is in the same units as the target variable, making it easier to understand in practical terms.\n",
        "\n",
        "11. What is pickling in Python, and how is it useful in ML?\n",
        "Pickling is a method to serialize Python objects. It's useful for saving trained models and reusing them later.\n",
        "\n",
        "12. What does a high R-squared value mean?\n",
        "It means the model explains a large portion of the variance in the dependent variable.\n",
        "\n",
        "13. What happens if linear regression assumptions are violated?\n",
        "Model performance and validity of inferences may degrade. For example, coefficients may be biased or standard errors misleading.\n",
        "\n",
        "14. How can we address multicollinearity in regression?\n",
        "\n",
        "*   Remove or combine correlated features\n",
        "*   Use PCA\n",
        "*   Apply regularization (Ridge, Lasso)\n",
        "\n",
        "15. How can feature selection improve model performance in regression analysis?\n",
        "It reduces overfitting, improves model interpretability, and increases computational efficiency.\n",
        "\n",
        "16. How is Adjusted R-squared calculated?\n",
        "Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
        "Where n = number of samples, p = number of predictors\n",
        "\n",
        "17. Why is MSE sensitive to outliers?\n",
        "Because it squares the errors, making large errors have an outsized effect.\n",
        "\n",
        "18. What is the role of homoscedasticity in linear regression?\n",
        "It ensures that residuals have equal variance across levels of the independent variables, which is important for valid inference.\n",
        "\n",
        "19. What is Root Mean Squared Error (RMSE)?\n",
        "RMSE is the square root of MSE and provides error in the same units as the target variable.\n",
        "\n",
        "20. Why is pickling considered risky?\n",
        "Pickled files can execute arbitrary code, making them insecure when loading from untrusted sources.\n",
        "\n",
        "21. What alternatives exist to pickling for saving ML models?\n",
        "\n",
        "*   joblib (optimized for large numpy arrays)\n",
        "*   ONNX\n",
        "*   PMML\n",
        "*   JSON (for model parameters)\n",
        "\n",
        "22. What is heteroscedasticity, and why is it a problem?\n",
        "Heteroscedasticity occurs when residuals have non-constant variance, violating regression assumptions and affecting model reliability.\n",
        "\n",
        "23. How can interaction terms enhance a regression model's predictive power?\n",
        "They capture relationships between features that affect the outcome together in a non-additive way."
      ],
      "metadata": {
        "id": "c2oY5ysvyWUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Visualize the distribution of residuals for multiple linear regression (Seaborn \"diamonds\" dataset)"
      ],
      "metadata": {
        "id": "LGmJYpI60Kog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and clean data\n",
        "df = sns.load_dataset('diamonds').dropna()\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "y = df['price']\n",
        "\n",
        "# Split and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Plot residuals\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rzII4w7zy6Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calculate and print MSE, MAE, and RMSE for a linear regression model"
      ],
      "metadata": {
        "id": "kRyQnXOO0cQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ],
      "metadata": {
        "id": "VpoAE9qj0WpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Check if the assumptions of linear regression are met"
      ],
      "metadata": {
        "id": "ShrzIH1J0YFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Linearity (pairplot)\n",
        "sns.pairplot(df[['carat', 'depth', 'table', 'price']])\n",
        "plt.suptitle(\"Linearity Check\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# 2. Homoscedasticity (residuals plot)\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Predicted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Homoscedasticity Check\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Multicollinearity (correlation matrix)\n",
        "corr_matrix = df[['carat', 'depth', 'table']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AvFe9HDs0Ynd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create an ML pipeline with feature scaling and evaluate regression models"
      ],
      "metadata": {
        "id": "ut_lqTTI0Ys8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "y = df['price']\n",
        "\n",
        "scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-validated R² scores:\", scores)\n",
        "print(\"Average R² score:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "UTfuQiTD0Yx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Simple linear regression model: print coefficients, intercept, R²"
      ],
      "metadata": {
        "id": "5FH5XYza0Y3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[['carat']]\n",
        "y = df['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"R-squared Score:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "Ar4Ga8B00Y9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Analyze relationship between total bill and tip (Seaborn 'tips' dataset)"
      ],
      "metadata": {
        "id": "Dd5Jirvp0ZDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tips = sns.load_dataset('tips')\n",
        "X = df_tips[['total_bill']]\n",
        "y = df_tips['tip']\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "plt.scatter(X, y, label='Actual')\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel(\"Total Bill\")\n",
        "plt.ylabel(\"Tip\")\n",
        "plt.title(\"Linear Regression: Total Bill vs Tip\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w9p4RQh90ZIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Fit linear regression on synthetic data and plot regression line"
      ],
      "metadata": {
        "id": "K5iYse030ZOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X.squeeze() + np.random.randn(100) * 2\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression().fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data')\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Synthetic Data with Regression Line\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kftorxwu0ZUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Pickle a trained linear regression model and save to file"
      ],
      "metadata": {
        "id": "2UvOW2j30ZZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Train on synthetic data from Q7\n",
        "with open(\"linear_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved to 'linear_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "BOA5aaCM0Zfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Fit a polynomial regression model (degree 2) and plot curve"
      ],
      "metadata": {
        "id": "ggTHOwq60Zk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Use synthetic data from Q7\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit model\n",
        "model_poly = LinearRegression().fit(X_poly, y)\n",
        "y_poly_pred = model_poly.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label=\"Data\")\n",
        "plt.plot(X, y_poly_pred, color='green', label=\"Polynomial Regression (deg 2)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Polynomial Regression Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "STXS1y5Y0ZqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Generate synthetic data for simple linear regression and print model"
      ],
      "metadata": {
        "id": "YwlCQPmn0Zv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.rand(100, 1) * 5\n",
        "y = 4 * X.squeeze() + np.random.randn(100)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "id": "w3M8eaZl0Z04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Fit polynomial regression models of different degrees and compare"
      ],
      "metadata": {
        "id": "55jjacPl0Z5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = [1, 2, 3, 4]\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "for d in degrees:\n",
        "    poly = PolynomialFeatures(degree=d)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    y_pred = model.predict(X_poly)\n",
        "    score = r2_score(y, y_pred)\n",
        "    print(f\"Degree {d} R-squared: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "CH5Sd-ea0Z-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Simple linear regression with two features: print coeffs, intercept, R²"
      ],
      "metadata": {
        "id": "2ZziilT00aES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['carat', 'table']]  # Use diamonds dataset\n",
        "y = df['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"R-squared Score:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "lPlO-Mh70aOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Generate synthetic data, fit linear regression, visualize with regression line"
      ],
      "metadata": {
        "id": "yaBQqvh60aUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression().fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data')\n",
        "plt.plot(X, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Linear Regression on Synthetic Data\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZtIXt_ad0aZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Use Variance Inflation Factor (VIF) to check for multicollinearity"
      ],
      "metadata": {
        "id": "BHowDV110ae1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Use diamonds dataset\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "X_const = add_constant(X)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = X_const.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_const.values, i)\n",
        "                   for i in range(X_const.shape[1])]\n",
        "\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "Af-RPia40akZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Generate polynomial data (degree 4), fit model, plot curve"
      ],
      "metadata": {
        "id": "1kstF6U00apb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 1*X.squeeze()**4 - 3*X.squeeze()**3 + 2*X.squeeze()**2 + np.random.randn(100) * 5\n",
        "\n",
        "# Polynomial regression\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data')\n",
        "plt.plot(X, y_pred, color='purple', label='Degree 4 Regression')\n",
        "plt.title(\"Polynomial Regression (Degree 4)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pQaQEtHZ0avd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. ML pipeline with standardization and multiple linear regression"
      ],
      "metadata": {
        "id": "WZfmZZo90a0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "X = df[['carat', 'depth', 'table']]\n",
        "y = df['price']\n",
        "\n",
        "pipeline.fit(X, y)\n",
        "r2 = pipeline.score(X, y)\n",
        "print(\"R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "id": "aN9UPnCN0a6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Polynomial regression (degree 3) on synthetic data"
      ],
      "metadata": {
        "id": "ibgvlofb0WOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.linspace(-2, 2, 100).reshape(-1, 1)\n",
        "y = X.squeeze()**3 - 2 * X.squeeze()**2 + 1 + np.random.randn(100)\n",
        "\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, label=\"Data\")\n",
        "plt.plot(X, y_pred, color=\"orange\", label=\"Degree 3 Regression\")\n",
        "plt.title(\"Polynomial Regression (Degree 3)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GB7QSMt51WBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Multiple linear regression on synthetic dataset with 5 features"
      ],
      "metadata": {
        "id": "ztM0FZXA1Vlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "model = LinearRegression().fit(X, y)\n",
        "\n",
        "print(\"R-squared Score:\", model.score(X, y))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "G_V0RxHN1ZKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Generate synthetic data for linear regression and visualize regression line"
      ],
      "metadata": {
        "id": "s9hqDzQo1Z5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 5 * X.squeeze() + np.random.randn(100) * 3\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression().fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label=\"Data\")\n",
        "plt.plot(X, y_pred, color=\"red\", label=\"Regression Line\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Linear Regression on Synthetic Data\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2TDC-PnR1cIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Create a synthetic dataset with 3 features and perform multiple linear regression"
      ],
      "metadata": {
        "id": "j6uliL2L1cvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate data\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=5, random_state=42)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "\n",
        "print(\"R-squared Score:\", model.score(X, y))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "JZplQbrF1c2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Serialize and deserialize ML models using joblib instead of pickle"
      ],
      "metadata": {
        "id": "8DssC9D51c7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, \"linear_model.joblib\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model = joblib.load(\"linear_model.joblib\")\n",
        "\n",
        "print(\"Model loaded successfully using joblib.\")\n"
      ],
      "metadata": {
        "id": "rn1lXZaB1dAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Perform linear regression with categorical features using one-hot encoding ('tips' dataset)"
      ],
      "metadata": {
        "id": "8ksjGw0l1dFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load data\n",
        "df = sns.load_dataset('tips')\n",
        "X = df[['total_bill', 'sex', 'smoker', 'day', 'time']]\n",
        "y = df['tip']\n",
        "\n",
        "# Preprocessing pipeline with one-hot encoding\n",
        "categorical = ['sex', 'smoker', 'day', 'time']\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('cat', OneHotEncoder(drop='first'), categorical)\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X, y)\n",
        "print(\"R-squared Score:\", pipeline.score(X, y))\n"
      ],
      "metadata": {
        "id": "12V434vP1dJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Compare Ridge Regression with Linear Regression on a synthetic dataset"
      ],
      "metadata": {
        "id": "eQeKf48u1dOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Use synthetic data from earlier\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Linear Regression\n",
        "lin_model = LinearRegression().fit(X, y)\n",
        "print(\"Linear Regression Coefficients:\", lin_model.coef_)\n",
        "print(\"Linear Regression R²:\", lin_model.score(X, y))\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0).fit(X, y)\n",
        "print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n",
        "print(\"Ridge Regression R²:\", ridge_model.score(X, y))\n"
      ],
      "metadata": {
        "id": "bfsCsMOz1dTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Use cross-validation to evaluate a Linear Regression model on synthetic data"
      ],
      "metadata": {
        "id": "XxN7NFeX1dYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=8, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(\"Cross-validated R² scores:\", scores)\n",
        "print(\"Average R² score:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "Ape3VXRC1dcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compare polynomial regression models of different degrees and print R-squared scores"
      ],
      "metadata": {
        "id": "rS9MFKU_18_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 0.5 * X.squeeze()**3 - X.squeeze()**2 + 2 * X.squeeze() + np.random.randn(100) * 2\n",
        "\n",
        "# Compare different polynomial degrees\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "for degree in degrees:\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    y_pred = model.predict(X_poly)\n",
        "\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    print(f\"Degree {degree} - R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "WkeYussU18Zm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}